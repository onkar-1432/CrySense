{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training and Fine-Tuning Sentence Transformers Models - Notebook Companion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Fine-Tune Sentence Transformers Models - Notebook Companion"
      ],
      "metadata": {
        "id": "I729oEbmU1rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZWjnEb3_XT0Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Sentence Transformers models work\n"
      ],
      "metadata": {
        "id": "P2CPLypZUwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "## Step 1: use an existing language model\n",
        "word_embedding_model = models.Transformer('distilroberta-base')\n",
        "\n",
        "## Step 2: use a pool function over the token embeddings\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "## Join steps 1 and 2 using the modules argument\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "metadata": {
        "id": "pixb3XSjXaqm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE59lryT3Mnj",
        "outputId": "f9a2459a-74ff-4846-97b4-94c863f70285"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to prepare your dataset for training a Sentence Transformers model\n"
      ],
      "metadata": {
        "id": "HYB2Vtvga1Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "1iMXl5_ka3Mf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.util import batch_to_device, fullname, is_datasets_available\n"
      ],
      "metadata": {
        "id": "gRehPGRg5fcK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lqIbiOD5c09",
        "outputId": "72819421-304b-4d43-dec8-48168134e0b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"embedding-data/QQP_triplets\"\n",
        "# dataset_id = \"embedding-data/sentence-compression\"\n",
        "\n",
        "dataset = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "max24luaa5OR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
        "print(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\n",
        "print(f\"- Examples look like this: {dataset['train'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUd11y8bP_a",
        "outputId": "591a892e-17ee-4c3a-dfe0-00edd5dbe5bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The embedding-data/QQP_triplets dataset has 101762 examples.\n",
            "- Each example is a <class 'dict'> with a <class 'dict'> as value.\n",
            "- Examples look like this: {'set': {'query': 'Why in India do we not have one on one political debate as in USA?', 'pos': ['Why cant we have a public debate between politicians in India like the one in US?'], 'neg': ['Can people on Quora stop India Pakistan debate? We are sick and tired seeing this everyday in bulk?', 'Why do politicians, instead of having a decent debate on issues going in and around the world, end up fighting always?', 'Can educated politicians make a difference in India?', 'What are some unusual aspects about politics and government in India?', 'What is debate?', 'Why does civic public communication and discourse seem so hollow in modern India?', 'What is a Parliamentary debate?', \"Why do we always have two candidates at the U.S. presidential debate. yet the ballot has about 7 candidates? Isn't that a misrepresentation of democracy?\", 'Why is civic public communication and discourse so hollow in modern India?', \"Aren't the Presidential debates teaching our whole country terrible communication skills and why is deliberate misrepresentation even allowed?\", 'Why are most Indian politicians uneducated?', 'Does Indian political leaders capable of doing face to face debates while running for office?', 'What is wrong with the Indian political system and the environment it has built in connection with the people of India? Have parties divided people more?', 'What is a debate?', 'Why do we have legislative council in india?', 'Why does the office of president of India, being politically neutral, not ask for Population control in India?', \"Why don't we discuss tax and foreign policies more in Indian elections but are instead obsessed with socialist schemes?\", 'Why do Indian politicians lack nationalist thinking?', 'Do you hate indian politicians?', 'Is India facing more stessful times and politically charged atmosphere when compared to Congress regime?', 'Who is the best politician in India? Why?', \"We all know about the present condition of Indian politicians; they are all just using us to run their train, but still, they win elections and rule over us. Why aren't people giving their vote to NOTA?\", 'Who are clean politicians in India?', 'Why are you not believing in Democracy of India?', 'What does politics in India mean? What are they actually doing?', 'What are the strongest arguments for a debate in favour of brain drain in India and what sources must be used for making a good short speech?', 'Do we really need an election commission in India?', 'Why is there no concept of political correctness in India? Is it a good thing or a bad thing?', 'Why is population control not on agenda of any political party in India?', 'Who are some of the most dangerous or worst politicians in India?']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the examples into `InputExample`s. It might around 10 seconds in Google Colab."
      ],
      "metadata": {
        "id": "-O2o_92kuzCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "train_examples = []\n",
        "train_data = dataset['train']['set']\n",
        "# For agility we only 1/2 of our available data\n",
        "n_examples = dataset['train'].num_rows // 2\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_data[i]\n",
        "  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))"
      ],
      "metadata": {
        "id": "yQ9v7x2J1C75"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.\")"
      ],
      "metadata": {
        "id": "j1XgafnCFHbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae62630-8742-4372-f3fd-0ba51e8d30cd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have a <class 'list'> of length 50881 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrap our training dataset into a Pytorch `Dataloader` to shuffle examples and get batch sizes."
      ],
      "metadata": {
        "id": "6qVYjIIOyppB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "id": "gbrXjod4dhJW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions for training a Sentence Transformers model\n"
      ],
      "metadata": {
        "id": "0HeVHzC-8bbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "train_loss = losses.TripletLoss(model=model)"
      ],
      "metadata": {
        "id": "t1gFBA6EFK10"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to train a Sentence Transformer model\n"
      ],
      "metadata": {
        "id": "QTSVaysgVHZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data"
      ],
      "metadata": {
        "id": "rBBR6OseFW0J"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training takes around 45 minutes with a Google Colab Pro account. Decrease the number of epochs and examples if you are using a free account or no GPU."
      ],
      "metadata": {
        "id": "Y17niC39k9S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n"
      ],
      "metadata": {
        "id": "6-XCDpCA5TFh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "7Ana59pTFjKn",
        "outputId": "a666d147-ec7e-4fb4-f4b3-811dae882513"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-784360fc8c04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_objectives=[(train_dataloader, train_loss)],\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           warmup_steps=warmup_steps) \n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/fit_mixin.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mtexts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf\"sentence_{idx}\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;31m# Add label column, unless all labels are 0 (the default value for `labels` in InputExample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0madd_label_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to share a Sentence Transformers to the Hugging Face Hub"
      ],
      "metadata": {
        "id": "uQEnJz5MVMPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iwjWb7HyTEX",
        "outputId": "adbe4d21-ca24-4232-c444-f872a11bce9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
            "\n",
            "git config --global credential.helper store\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_to_hub(\n",
        "    \"distilroberta-base-sentence-transformer\",\n",
        "    organization=\"embedding-data\",\n",
        "    train_datasets=[\"embedding-data/QQP_triplets\"],\n",
        "    exist_ok=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "sM6pu_adyUw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: How to fine-tune a Sentence Transformer model\n"
      ],
      "metadata": {
        "id": "Rq2ROpPUVSS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will fine-tune our Sentence Transformer model."
      ],
      "metadata": {
        "id": "iErisVnE5sCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelB = SentenceTransformer('embedding-data/distilroberta-base-sentence-transformer')"
      ],
      "metadata": {
        "id": "JPEqCxxr0REd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = \"embedding-data/sentence-compression\"\n",
        "datasetB = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "akDLR6IS0sdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Examples look like this: {datasetB['train']['set'][0]}\")"
      ],
      "metadata": {
        "id": "fwYA76vY2YbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examplesB = []\n",
        "train_dataB = dataset['train']['set']\n",
        "n_examples = dataset['train'].num_rows\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_dataB[i]\n",
        "  train_examplesB.append(InputExample(texts=[example[0], example[1]]))"
      ],
      "metadata": {
        "id": "2s1c05zF2g7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloaderB = DataLoader(train_examplesB, shuffle=True, batch_size=64)\n",
        "train_lossB = losses.MultipleNegativesRankingLoss(model=modelB)\n",
        "num_epochsB = 10\n",
        "warmup_stepsB = int(len(train_dataloaderB) * num_epochsB * 0.1) #10% of train data"
      ],
      "metadata": {
        "id": "cMYfDAbd53xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.fit(train_objectives=[(train_dataloaderB, train_lossB)],\n",
        "          epochs=num_epochsB,\n",
        "          warmup_steps=warmup_stepsB)"
      ],
      "metadata": {
        "id": "lfRC496k6hS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.save_to_hub(\n",
        "    \"distilroberta-base-sentence-transformer\",\n",
        "    organization=\"embedding-data\",\n",
        "    train_datasets=[\"embedding-data/sentence-compression\"],\n",
        "    exist_ok=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "Ilq1kA6Rbboh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}